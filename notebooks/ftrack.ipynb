{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c912687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cf521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelFactoredEnv():\n",
    "    def __init__(self, k, d, num_trials, sigma=0.01, min_expected=0.3, max_expected=1, seed=0):\n",
    "        self.d = d\n",
    "        self.num_trials = num_trials\n",
    "        self.sigma=sigma\n",
    "        self.d_vect = np.linspace(0, d-1, d, dtype=int)\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.avg_reward = np.random.uniform(min_expected, max_expected, (self.num_trials, self.d, k))\n",
    "    \n",
    "    def step(self, trial, action):\n",
    "        return self.avg_reward[trial, self.d_vect, action] + np.random.normal(0, self.sigma, self.d)\n",
    "\n",
    "    def get_expected(self, trial):\n",
    "        return self.avg_reward[trial, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2522ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoredUCBAgent():\n",
    "    \"\"\"\n",
    "    This class implements the FRB algorithm in its anytime version\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms_vect, dim, sigma, max_reward=1, exploration_alpha=4):\n",
    "        self.n_arms_vect = n_arms_vect\n",
    "        self.dim = dim\n",
    "        assert self.dim == self.n_arms_vect.shape[0]\n",
    "        self.max_reward = max_reward\n",
    "        self.sigma = sigma\n",
    "        self.exploration_alpha = exploration_alpha\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 1\n",
    "        self.last_pull = None\n",
    "        self.avg_reward = []\n",
    "        self.n_pulls = []\n",
    "        for size in self.n_arms_vect:\n",
    "            self.avg_reward.append(np.zeros(size))\n",
    "            self.n_pulls.append(np.zeros(size, dtype=int))\n",
    "        return self\n",
    "\n",
    "    def pull_arm(self):\n",
    "        self.last_pull = -1 * np.ones(self.dim, dtype=int)\n",
    "        for i in range(self.dim):\n",
    "            ucb1 = [self.avg_reward[i][a] + self.max_reward * self.sigma * np.sqrt(\n",
    "                self.exploration_alpha * np.log(self.t) / self.n_pulls[i][a]) for a in range(self.n_arms_vect[i])]\n",
    "            self.last_pull[i] = int(np.argmax(ucb1))\n",
    "            self.n_pulls[i][self.last_pull[i]] = self.n_pulls[i][self.last_pull[i]] + 1\n",
    "        return self.last_pull\n",
    "\n",
    "    def update(self, observations):\n",
    "        self.t += 1\n",
    "        for i in range(self.dim):\n",
    "            self.avg_reward[i][self.last_pull[i]] = (\n",
    "                self.avg_reward[i][self.last_pull[i]] *\n",
    "                (self.n_pulls[i][self.last_pull[i]] - 1) + observations[i]\n",
    "            ) / (self.n_pulls[i][self.last_pull[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3512e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FtrackAgent():\n",
    "    \"\"\"\n",
    "    This class implements F-track\n",
    "    \"\"\"\n",
    "    def __init__(self, k, d, sigma, T, c):\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.c = c\n",
    "        self.N0 = 10 * int(np.ceil(np.sqrt(np.log(T))))\n",
    "        self.eps = np.sqrt(2 * (sigma ** 2) * self._ft(1/np.log(T), c) / self.N0)\n",
    "        self.exploration_alpha = 4\n",
    "        self.schedule = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.last_pull = None\n",
    "        self.avg_reward = np.zeros((self.d, self.k))\n",
    "        self.n_pulls = np.zeros((self.d, self.k), dtype=int)\n",
    "        # self.n_arms_vect = self.k * np.ones(self.d, dtype=int)\n",
    "        # for size in self.n_arms_vect:\n",
    "        #     self.avg_reward.append(np.zeros(size))\n",
    "        #     self.n_pulls.append(np.zeros(size, dtype=int))\n",
    "        return self\n",
    "\n",
    "    def pull_arm(self):\n",
    "        if(self.t < self.N0*self.k): \n",
    "            self.last_pull = (self.t % self.k) * np.ones(self.d, dtype=int)\n",
    "        else:\n",
    "            if self.schedule == False:\n",
    "                self._create_schedule()\n",
    "        \n",
    "            if np.max(np.abs(self.avg_rewards_warmup-self.avg_reward)) <= 2 * self.eps:\n",
    "                self._pull_arm_ftrack()\n",
    "            else:\n",
    "                self._pull_arm_fucb()\n",
    "            \n",
    "        for i in range(self.d):\n",
    "            self.n_pulls[i, self.last_pull[i]] = self.n_pulls[i, self.last_pull[i]] + 1\n",
    "\n",
    "        return self.last_pull\n",
    "    \n",
    "    def _pull_arm_ftrack(self):\n",
    "        finished = self.action_vects_num == self.action_vects_num_pulled\n",
    "        self.action_vects_num_pulled[finished] = np.inf\n",
    "        to_pull = np.argmin(self.action_vects_num_pulled)\n",
    "        self.last_pull = self.action_vects[to_pull]\n",
    "        self.action_vects_num_pulled[to_pull] = self.action_vects_num_pulled[to_pull] + 1\n",
    "    \n",
    "    def _pull_arm_fucb(self):\n",
    "        self.last_pull = -1 * np.ones(self.d, dtype=int)\n",
    "        for i in range(self.d):\n",
    "            ucb1 = [self.avg_reward[i, a] + self.sigma * np.sqrt(\n",
    "                self.exploration_alpha * np.log(self.t) / self.n_pulls[i, a]) for a in range(self.k)]\n",
    "            self.last_pull[i] = int(np.argmax(ucb1))\n",
    "            # self.n_pulls[i, self.last_pull[i]] = self.n_pulls[i, self.last_pull[i]] + 1\n",
    "\n",
    "    def update(self, observations):\n",
    "        self.t += 1\n",
    "        for i in range(self.d):\n",
    "            self.avg_reward[i, self.last_pull[i]] = (\n",
    "                self.avg_reward[i, self.last_pull[i]] *\n",
    "                (self.n_pulls[i, self.last_pull[i]] - 1) + observations[i]\n",
    "            ) / (self.n_pulls[i, self.last_pull[i]])\n",
    "    \n",
    "    def _ft(self, delta, c):\n",
    "        return (1 + 1 / np.log(self.T)) * (c * np.log(np.log(self.T)) + np.log(1/delta))\n",
    "    \n",
    "    def _create_schedule(self):\n",
    "        self.avg_rewards_warmup = np.copy(self.avg_reward)\n",
    "        max_val = np.max(self.avg_rewards_warmup, axis=1).reshape(self.d, -1)\n",
    "        max_idx = np.argmax(self.avg_rewards_warmup, axis=1)\n",
    "        deltas = max_val - self.avg_rewards_warmup\n",
    "        self.pulls_todo = np.zeros((self.d, self.T - self.N0*self.k))\n",
    "        ft = self._ft(1/self.T, self.c)\n",
    "        for i in range(self.d):\n",
    "            self.pulls_todo[i, :] = max_idx[i]\n",
    "            N = np.ceil(2 * (self.sigma ** 2) * ft / (deltas[i, :] ** 2)).astype(int)\n",
    "            order = np.argsort(N)\n",
    "            N_ordered = N[order]\n",
    "            counter = 0\n",
    "            for j in range(self.k-1):\n",
    "                self.pulls_todo[i, counter:counter + N_ordered[j]] = order[j]\n",
    "                counter += N_ordered[j]\n",
    "        self.action_vects = []\n",
    "        self.action_vects_num = []\n",
    "        for i in range(self.T - self.N0*self.k):\n",
    "            if (i == 0):\n",
    "                self.action_vects.append(self.pulls_todo[:, 0])\n",
    "                self.action_vects_num.append(1)\n",
    "            if np.array_equal(self.pulls_todo[:, i], self.action_vects[-1]):\n",
    "                self.action_vects_num[-1] = self.action_vects_num[-1] + 1\n",
    "            else:\n",
    "                self.action_vects.append(self.pulls_todo[:, i])\n",
    "                self.action_vects_num.append(1)\n",
    "        self.action_vects_num_pulled = list(np.zeros(len(self.action_vects_num)))     \n",
    "        self.schedule = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8bd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Current Directory from /home/simone/Projects/research/FRB/notebooks to /home/simone/Projects/research/FRB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alg \u001b[38;5;241m==\u001b[39m ftrack:\n\u001b[0;32m--> 107\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43magent_stellina\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpull_arm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m         agent_stellina\u001b[38;5;241m.\u001b[39mupdate(env\u001b[38;5;241m.\u001b[39mstep(action))\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m alg \u001b[38;5;241m==\u001b[39m fucb:\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mFtrackAgent.pull_arm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_rewards_warmup\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_reward)) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pull_arm_ftrack()\n",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m, in \u001b[0;36mFtrackAgent._create_schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_vects\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpulls_todo[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_vects_num\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpulls_todo[:, i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_vects[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_vects_num[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_vects_num[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# -*- coding: latin-1 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib as tkz\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, sys\n",
    "\n",
    "_, filename = os.path.split(os.getcwd())\n",
    "if filename == 'notebooks':\n",
    "    old_dir = os.getcwd()\n",
    "    os.chdir('../')\n",
    "    print('Moving Current Directory from ' + old_dir + ' to ' + os.getcwd())\n",
    "else:\n",
    "    print('Current Directory is ' + os.getcwd())\n",
    "\n",
    "sys.path.append('./')\n",
    "\n",
    "from FRB.agents import FactoredUCBAgent\n",
    "from FRB.env import FactoredEnv\n",
    "from FRB.utils import get_pulled_expected, compute_max_expected, create_action_matrix, get_sigma_square_eq_max\n",
    "\n",
    "# Body of the script\n",
    "\n",
    "# BASIC SETTING FOR EXPERIMENTS\n",
    "fucb = '\\\\JPAalgnameshort'\n",
    "ftrack = '\\\\FTrack'\n",
    "algs = [fucb, ftrack]\n",
    "checkpoints = [1000, 5000, 10000]\n",
    "n_trials = 50\n",
    "seed = 0\n",
    "k_list = [3]\n",
    "d_list = [2]\n",
    "# k_list = [int(sys.argv[1])]\n",
    "# d_list = [int(sys.argv[2])]\n",
    "T = 10000 #int(sys.argv[3])\n",
    "bounded_list = [False]\n",
    "do_subsampling = True\n",
    "\n",
    "# OVERRIDE FOR TESTING PURPOSE TO SPEED UP THE RUNS\n",
    "T = 10000\n",
    "checkpoints = [1000, 2000, 5000]\n",
    "bounded_list = [False]\n",
    "# algs = [fucb, tea]\n",
    "n_trials = 4\n",
    "k_list = [3]\n",
    "d_list = [2]\n",
    "do_subsampling = True\n",
    "    \n",
    "result_table = {}\n",
    "# out_folder = str(sys.argv[4])\n",
    "\n",
    "# ht_mult = float(sys.argv[5])\n",
    "_sigma = 0.05 #float(sys.argv[6])\n",
    "\n",
    "for bounded in bounded_list:\n",
    "\n",
    "    result_table[bounded] = {}\n",
    "    \n",
    "    if bounded: \n",
    "        sigma = 0.5 # fixed for bernoulli\n",
    "    else:\n",
    "        sigma = _sigma\n",
    "    \n",
    "    for d in d_list:\n",
    "\n",
    "        result_table[bounded][d] = {}\n",
    "\n",
    "        for k in k_list:\n",
    "\n",
    "            # out_path = out_folder + 'out' + str(k) + '_' + str(d) + '.txt'\n",
    "\n",
    "            # result_table[bounded][d][k] = {}\n",
    "\n",
    "            arms_vect = k * np.ones(d, dtype=int)\n",
    "\n",
    "            # F-UCB INIT\n",
    "            agent_factored = FactoredUCBAgent(arms_vect, d, sigma)\n",
    "\n",
    "            agent_stellina = FtrackAgent(k, d, sigma, T, c=2.5)\n",
    "            \n",
    "            mean_cum_expected_regret = {}\n",
    "            std_cum_expected_regret = {}\n",
    "            \n",
    "            for alg in algs:\n",
    "\n",
    "                # result_table[bounded][d][k][alg] = {}\n",
    "\n",
    "                env = FactoredEnv(arms_vect, d, sigma=sigma, bounded=bounded)\n",
    "\n",
    "                inst_expected_regret = np.zeros((n_trials, T))\n",
    "                \n",
    "                # for trial_i in range(n_trials):\n",
    "                for trial_i in tqdm(range(n_trials)):\n",
    "                \n",
    "                    vals_expected = env.get_expected()\n",
    "                    max_expected = compute_max_expected(vals_expected)\n",
    "\n",
    "                    for t in range(T):\n",
    "\n",
    "                        if alg == ftrack:\n",
    "                            action = agent_stellina.pull_arm()\n",
    "                            agent_stellina.update(env.step(action))\n",
    "                        elif alg == fucb:\n",
    "                            action = agent_factored.pull_arm()\n",
    "                            agent_factored.update(env.step(action))\n",
    "                        else:\n",
    "                            raise ValueError('Error in selecting algorithm')\n",
    "\n",
    "                        inst_expected_regret[trial_i, t] = max_expected - get_pulled_expected(\n",
    "                            vals_expected, action)\n",
    "                    \n",
    "                    # I reset all the agents, becuase i do not know which one \n",
    "                    # i am using for the sake of simplicity\n",
    "                    agent_factored.reset()\n",
    "                    agent_stellina.reset()\n",
    "                \n",
    "                # maybe replace with cumsum with correct axis\n",
    "                cum_expected_regret = np.zeros(inst_expected_regret.shape)\n",
    "                cum_expected_regret[:, 0] = inst_expected_regret[:, 0]\n",
    "                for i in range(1, T):\n",
    "                    cum_expected_regret[:, i] = inst_expected_regret[:, i] + cum_expected_regret[:, i-1]\n",
    "\n",
    "                mean_cum_expected_regret[alg] = np.mean(cum_expected_regret, axis=0)\n",
    "                std_cum_expected_regret[alg] = np.std(cum_expected_regret, axis=0) / np.sqrt(n_trials)\n",
    "\n",
    "            plt.figure()\n",
    "            if do_subsampling:\n",
    "                subsample = 50\n",
    "                assert T % subsample == 0\n",
    "                x_plt = np.linspace(0, T-1, int(T/subsample), dtype=int)\n",
    "            else:\n",
    "                x_plt = np.linspace(0, T-1, T, dtype=int)\n",
    "            for alg in algs:\n",
    "                plt.plot(x_plt, mean_cum_expected_regret[alg][x_plt], \n",
    "                         label=alg)\n",
    "                plt.fill_between(x_plt, \n",
    "                                 mean_cum_expected_regret[alg][x_plt] - std_cum_expected_regret[alg][x_plt], \n",
    "                                 mean_cum_expected_regret[alg][x_plt] + std_cum_expected_regret[alg][x_plt], \n",
    "                                 alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.xlabel('Rounds')\n",
    "            plt.ylabel('Regret')\n",
    "            plt.title('bounded={} k={} d={} $\\sigma$={}'.format(bounded, k, d, sigma))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
